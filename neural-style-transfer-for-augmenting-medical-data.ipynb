{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction + Set-up\n","\n","TensorFlow is a powerful tool to develop any machine learning pipeline. This model explores the concept of using neural style transfer to augment medical images.\n","\n","Medical images are rarely ever processed in the same way. Different institutions, different machines, different technicians, and many other factors lead to variation within medical scans. These variations can impact how well an ML model can learn from given data. If images processed by institution #1 work better with the model than images processed by institution #2, we would want our institution #2 images to look more like institution #1 images.\n","\n","Neural style transfer is outlined in [\"A Neural Algorithm of Artistic Style\"](https://arxiv.org/abs/1508.06576) (Gatys et al.). On a very high level, neural style transfer is used to compose on image in the style of another using deep learning. This notebook shows that neural style transfer does indeed improve scores for images processed by lower-scoring methods."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)\n","ERROR: No matching distribution found for tensorflow==2.2.0\n"]}],"source":["! pip install tensorflow==2.2.0 -q"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'kaggle_datasets'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\91739\\Downloads\\panda\\panda\\neural-style-transfer-for-augmenting-medical-data.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/91739/Downloads/panda/panda/neural-style-transfer-for-augmenting-medical-data.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/91739/Downloads/panda/panda/neural-style-transfer-for-augmenting-medical-data.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/91739/Downloads/panda/panda/neural-style-transfer-for-augmenting-medical-data.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkaggle_datasets\u001b[39;00m \u001b[39mimport\u001b[39;00m KaggleDatasets\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/91739/Downloads/panda/panda/neural-style-transfer-for-augmenting-medical-data.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/91739/Downloads/panda/panda/neural-style-transfer-for-augmenting-medical-data.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m SEED \u001b[39m=\u001b[39m \u001b[39m1337\u001b[39m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_datasets'"]}],"source":["import os\n","import PIL\n","import math\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from kaggle_datasets import KaggleDatasets\n","from sklearn.model_selection import train_test_split\n","\n","SEED = 1337\n","print('Tensorflow version : {}'.format(tf.__version__))\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except ValueError:\n","    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n","    \n","print('Number of replicas:', strategy.num_replicas_in_sync)"]},{"cell_type":"markdown","metadata":{},"source":["# Data loading\n","\n","The first step is to load in our data. We will be working with the PANDA data that was used for the Prostate Cancer Grade Assessment Challenge. The tiling for this dataset is explained in my first [PANDA notebook](https://www.kaggle.com/amyjang/tensorflow-cnn-data-augmentation-prostate-cancer). The tiles have been aggregated into singular images for your convenience in a new [dataset](https://www.kaggle.com/amyjang/pandatilesagg).\n","\n","The PANDA data contains prostate cancer microscopy scans from two different institutions - Radboud University Medical Center and Karolinska Institute. We'll run our ML classifying model on the two sets of images separately to see which institution has better performing data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MAIN_DIR = '../input/pandatilesagg'\n","TRAIN_IMG_DIR = os.path.join(MAIN_DIR, 'all_images')\n","train_csv = pd.read_csv(os.path.join(MAIN_DIR, 'train.csv'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["radboud_csv = train_csv[train_csv['data_provider'] == 'radboud']\n","karolinska_csv = train_csv[train_csv['data_provider'] != 'radboud']\n","img_ids = train_csv['image_id']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["r_train, r_test = train_test_split(\n","    radboud_csv,\n","    test_size=0.2, random_state=SEED\n",")\n","\n","k_train, k_test = train_test_split(\n","    karolinska_csv,\n","    test_size=0.2, random_state=SEED\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Generally, it is better practice to specify constant variables than it is to hard-code numbers. This way, changing parameters is more efficient and complete. Specfiy some constants below."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","IMG_DIM = (1536, 128)\n","CLASSES_NUM = 6\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","EPOCHS = 100\n","N=12"]},{"cell_type":"markdown","metadata":{},"source":["The first step is to create a function that will decode our image into a tensor. We can use the `tf.image` API."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def decode_img(img):\n","  # convert the compressed string to a 3D uint8 tensor\n","  img = tf.image.decode_jpeg(img, channels=3)\n","  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n","  img = tf.image.convert_image_dtype(img, tf.float32)\n","  # resize the image to the desired size.\n","  return tf.image.resize(img, IMG_DIM)"]},{"cell_type":"markdown","metadata":{},"source":["This second function maps the filepath to the the image and label. The files are labeled so that the label - the ISUP grade - is the first part of the file name and the image id is the second part."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_item(file_path):    \n","    image = tf.io.read_file(file_path)\n","    image = decode_img(image)\n","    label = tf.strings.split(file_path, '_')\n","    label = tf.strings.to_number(label[-2])\n","    label = tf.cast(label, tf.int32)\n","    \n","    return image, tf.one_hot(label, CLASSES_NUM)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["r_train['isup_grade'] = r_train['isup_grade'].apply(str)\n","r_train['file'] = TRAIN_IMG_DIR +'/_' + r_train['isup_grade'] + '_' + r_train['image_id'] + '.jpg'\n","\n","r_test['isup_grade'] = r_test['isup_grade'].apply(str)\n","r_test['file'] = TRAIN_IMG_DIR +'/_' + r_test['isup_grade'] + '_' + r_test['image_id'] + '.jpg'\n","\n","k_train['isup_grade'] = k_train['isup_grade'].apply(str)\n","k_train['file'] = TRAIN_IMG_DIR +'/_' + k_train['isup_grade'] + '_' + k_train['image_id'] + '.jpg'\n","\n","k_test['isup_grade'] = k_test['isup_grade'].apply(str)\n","k_test['file'] = TRAIN_IMG_DIR +'/_' + k_test['isup_grade'] + '_' + k_test['image_id'] + '.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_dataset(df):\n","    ds = tf.data.Dataset.from_tensor_slices(df['file'].values)\n","    ds = ds.map(get_item, num_parallel_calls=AUTOTUNE)\n","    ds = ds.shuffle(buffer_size=1000)\n","    ds = ds.batch(BATCH_SIZE)\n","    ds = ds.prefetch(buffer_size=AUTOTUNE)\n","    return ds"]},{"cell_type":"markdown","metadata":{},"source":["We're going to define two different datasets: one with radboud images and one with the karolinska images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["r_train_ds = get_dataset(r_train)\n","k_train_ds = get_dataset(k_train)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize our input data\n","\n","Run the following cell to define the method to visualize our input data. This method displays the new images and their corresponding ISUP grade."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_batch(image_batch, label_batch):\n","    plt.figure(figsize=(10,10))\n","    for n in range(10):\n","        ax = plt.subplot(1,10,n+1)\n","        plt.imshow(image_batch[n])\n","        plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["**Radboud Scans**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_batch, label_batch = next(iter(r_train_ds))\n","show_batch(image_batch, label_batch)"]},{"cell_type":"markdown","metadata":{},"source":["**Karolinska Scans**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_batch, label_batch = next(iter(k_train_ds))\n","show_batch(image_batch, label_batch)"]},{"cell_type":"markdown","metadata":{},"source":["We see from these two visualizations that the Radboud and Karolinska images look very different. Even from a quick initial glance, we se that the hues and colors are different. Will the variations affect how well the images do in our model?"]},{"cell_type":"markdown","metadata":{},"source":["# Build our classifying model\n","\n","We will be utilizing the VGG16 pre-trained model to classify our data. Since the base model has already been trained with imagenet weights, we do not want the weights to change, so the base mode must not be trainable. However, the number of classes that our model has differs from the original model. Therefore, we do not want to include the top layers because we will add our own Dense layer that has the same number of nodes as our output class."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_model():\n","    base_model = tf.keras.applications.VGG16(input_shape=(*IMG_DIM, 3),\n","                                             include_top=False,\n","                                             weights='imagenet')\n","    \n","    base_model.trainable = True\n","    \n","    model = tf.keras.Sequential([\n","        base_model,\n","        \n","        tf.keras.layers.GlobalAveragePooling2D(),\n","        tf.keras.layers.Dense(16, activation='relu'),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dense(CLASSES_NUM, activation='softmax'),\n","    ])\n","    \n","    model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n","                  loss='categorical_crossentropy',\n","                  metrics=[tf.keras.metrics.AUC(name='auc')])\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["Learning rate is a very important hyperparameter, and it can be difficult to choose the \"right\" one. A learning rate that it too high will prevent the model from converging, but one that is too low will be far too slow. We will utilize multiple callbacks, using the `tf.keras` API to make sure that we are using an ideal learning rate and to prevent the model from overfitting. We can also save our model so that we do not have to retrain it next time."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def exponential_decay(lr0, s):\n","    def exponential_decay_fn(epoch):\n","        return lr0 * 0.1 **(epoch / s)\n","    return exponential_decay_fn\n","\n","exponential_decay_fn = exponential_decay(0.01, 20)\n","\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n","\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n","                                                     restore_best_weights=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Compare scores between institutions\n","\n","Let's run our model on the Radboud images first."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    model = make_model()\n","\n","history = model.fit(\n","    r_train_ds, epochs=20,\n","    callbacks=[early_stopping_cb, lr_scheduler]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The ROC AUC score for the radboud images are quite low, near 0.5. This means that the the model finds it difficult to distinguish between the images of the different class."]},{"cell_type":"markdown","metadata":{},"source":["Let's run the same model on the Karolinska images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    model = make_model()\n","    \n","history = model.fit(\n","    k_train_ds, epochs=20,\n","    callbacks=[early_stopping_cb, lr_scheduler]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The ROC AUC scores are way higher for the karolinska images. This can be a problem because the discrepancy shows that the images from the different institutions do in fact have noticable variances by the machine. It can also cause the model to be biased towards karolinska images, and this can hurt patients with radboud images.\n","\n","The idea is that we will use neural style transfer to change radboud images to look more like karolinska images."]},{"cell_type":"markdown","metadata":{},"source":["# Neural style transfer intro\n","\n","Let's first visualize a radboud image and a karolinska image next to each other.\n","\n","A more in-depth tutorial of neural style transfer for artistic painting transformations can be found [here](https://www.tensorflow.org/tutorials/generative/style_transfer)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_batch, _ = next(iter(r_train_ds))\n","r_image = image_batch[4].numpy()\n","plt.figure(figsize=(10,10))\n","ax = plt.subplot(1,2,1)\n","plt.title(\"radboud\")\n","plt.imshow(r_image)\n","plt.axis(\"off\")\n","\n","image_batch, _ = next(iter(k_train_ds))\n","k_image = image_batch[4].numpy()\n","ax = plt.subplot(1,2,2)\n","plt.imshow(k_image)\n","plt.title(\"karolinska\")\n","plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["The differences in the processing techniques is apparent.\n","\n","We will be using our karolinska image as our style image because we want to change our radboud images to look more like our karolinska images. Our model takes in 4D tensors, so let's first reformat our style and content images. Before we restyle all of our radboud images, let's first visualize how the neural style transfer works on a single image."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["content_image = tf.expand_dims(r_image, axis = 0)\n","style_image = tf.expand_dims(k_image, axis = 0)"]},{"cell_type":"markdown","metadata":{},"source":["# Define style and content layers\n","\n","In a deep convolution neural net, the lower layers capture lower-level features like texture and higher layers capture higher-level features such as shapes. We define the content layers and style layers below."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["content_layers = ['block5_conv2'] \n","\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1']\n","\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)"]},{"cell_type":"markdown","metadata":{},"source":["# Build the neural style transfer model\n","\n","Our next step is to define a model that returns a list of intermediate layer outputs for our VGG19 model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def vgg_layers(layer_names):\n","  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n","  # Load our model. Load pretrained VGG, trained on imagenet data\n","  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","  vgg.trainable = False\n","  \n","  outputs = [vgg.get_layer(name).output for name in layer_names]\n","\n","  model = tf.keras.Model([vgg.input], outputs)\n","  return model"]},{"cell_type":"markdown","metadata":{},"source":["We can extract our style features, defined by the style layers we chose earlier, and convert them to style outputs. Since our inputs are values between 0 and 255, we have to multiple the style features by 255."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["style_extractor = vgg_layers(style_layers)\n","style_outputs = style_extractor(style_image*255)"]},{"cell_type":"markdown","metadata":{},"source":["# Calculate style\n","\n","Style is a rather arbitrary concept, and we want to convert it into a value that can be understood by the model. Style can be calculated by a gram matrix below."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def gram_matrix(input_tensor):\n","  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n","  input_shape = tf.shape(input_tensor)\n","  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n","  return result/(num_locations)"]},{"cell_type":"markdown","metadata":{},"source":["Build a model that extracts style and content."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class StyleContentModel(tf.keras.models.Model):\n","  def __init__(self, style_layers, content_layers):\n","    super(StyleContentModel, self).__init__()\n","    self.vgg =  vgg_layers(style_layers + content_layers)\n","    self.style_layers = style_layers\n","    self.content_layers = content_layers\n","    self.num_style_layers = len(style_layers)\n","    self.vgg.trainable = False\n","\n","  def call(self, inputs):\n","    \"Expects float input in [0,1]\"\n","    inputs = inputs*255.0\n","    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","    outputs = self.vgg(preprocessed_input)\n","    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n","                                      outputs[self.num_style_layers:])\n","\n","    style_outputs = [gram_matrix(style_output)\n","                     for style_output in style_outputs]\n","\n","    content_dict = {content_name:value \n","                    for content_name, value \n","                    in zip(self.content_layers, content_outputs)}\n","\n","    style_dict = {style_name:value\n","                  for style_name, value\n","                  in zip(self.style_layers, style_outputs)}\n","    \n","    return {'content':content_dict, 'style':style_dict}"]},{"cell_type":"markdown","metadata":{},"source":["The StyleContentModel will return the gram matrix of the style layers and content layers."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["extractor = StyleContentModel(style_layers, content_layers)\n","\n","results = extractor(tf.constant(content_image))"]},{"cell_type":"markdown","metadata":{},"source":["The way that loss is calculated is by calculating mean square error of the model outputs to the targets, defined below. The weight of the losses (style vs. content) are defined below."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["style_targets = extractor(style_image)['style']\n","content_targets = extractor(content_image)['content']"]},{"cell_type":"markdown","metadata":{},"source":["We'll define a clipping method to keep the values between 0 and 1."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def clip_0_1(image):\n","  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n","\n","opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"]},{"cell_type":"markdown","metadata":{},"source":["Define the weights. Higher the weight, the more important the loss will be calculated."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["style_weight=1e-1\n","content_weight=1e4"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def style_content_loss(outputs):\n","    style_outputs = outputs['style']\n","    content_outputs = outputs['content']\n","    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n","                           for name in style_outputs.keys()])\n","    style_loss *= style_weight / num_style_layers\n","\n","    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n","                             for name in content_outputs.keys()])\n","    content_loss *= content_weight / num_content_layers\n","    loss = style_loss + content_loss\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["Define the training step."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_step(image):\n","  with tf.GradientTape() as tape:\n","    outputs = extractor(image)\n","    loss = style_content_loss(outputs)\n","\n","  grad = tape.gradient(loss, image)\n","  opt.apply_gradients([(grad, image)])\n","  image.assign(clip_0_1(image))"]},{"cell_type":"markdown","metadata":{},"source":["# Train the neural style model\n","\n","Because we aren't using a typical Keras model, we cannot run model.fit. Instead, we'll define our own training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["steps_per_epoch = 3\n","\n","def style_transfer(image):\n","    image = tf.expand_dims(image, axis = 0)\n","    image = tf.Variable(lambda : image)\n","    step = 0\n","    for m in range(steps_per_epoch):\n","        step += 1\n","        train_step(image)\n","        print(\".\", end='')\n","        \n","    return image"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize the neural style transfer\n","\n","Before we augment all of the images in our dataset, let's fist visualize how our augmented radboud image compared to our original one."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","ax = plt.subplot(1,2,1)\n","plt.title(\"radboud\")\n","plt.imshow(r_image)\n","plt.axis(\"off\")\n","\n","r_aug_image = style_transfer(r_image)\n","ax = plt.subplot(1,2,2)\n","plt.imshow(r_aug_image[0])\n","plt.title(\"radboud augmented\")\n","plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["We see that there a style transfer did in fact occur. Running the neural style transfer may change the original image too much, and changing the higher-level features may hurt classifiction. Therefore, we want to keep the number of epochs relatively low."]},{"cell_type":"markdown","metadata":{},"source":["# Run neural style transfer on Radboud images\n","\n","Now that we tested to see that neural style transfer works on a single radboud image, we want to run neural style on all the radboud images. Because of resource limitations, the code is commented out below. However, the output for the data augmentation is under the radboud_aug directory in the dataset.\n","\n","As a note, not all the radboud images have been augmented for efficiency purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ! mkdir images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for file in r_train['file'].values:\n","#     img = np.array(PIL.Image.open(file)) /255.0\n","#     img = style_transfer(img)[0] * 255.0\n","#     img = img.numpy()\n","#     im = img.astype('uint8')\n","#     im = PIL.Image.fromarray(im)\n","#     im.save(\"images/\" + file.split('/')[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import shutil\n","# shutil.make_archive(\"images\", 'zip', \"/kaggle/working/images\")"]},{"cell_type":"markdown","metadata":{},"source":["# Load augmented images and train the model\n","\n","Let's load in our augmented radboud images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_dataset(ds):\n","    ds = ds.map(get_item, num_parallel_calls=AUTOTUNE)\n","    ds = ds.shuffle(buffer_size=1000)\n","    ds = ds.batch(BATCH_SIZE)\n","    ds = ds.prefetch(buffer_size=AUTOTUNE)\n","    return ds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rad_aug = tf.data.Dataset.list_files(\"../input/pandatilesagg/radboud_aug/*\")\n","rad_aug = get_dataset(rad_aug)"]},{"cell_type":"markdown","metadata":{},"source":["Visualize the augmented radboud images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_batch, label_batch = next(iter(rad_aug))\n","show_batch(image_batch, label_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    model = make_model()\n","    \n","history = model.fit(\n","    rad_aug, epochs=20,\n","    callbacks=[early_stopping_cb, lr_scheduler]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The ROC AUC score for the augmented images score a lot higher than the original radboud score.\n","\n","From 0.54 --> 0.67!! (The exact values may be different because we did not specify a random seed)\n","\n","The other model does not shown signs of improvement after each epoch, but using augmented images shows visible improvements. This notebook highlights neural style transfer's potential to augment images in the medical space!!\n","\n","This idea has many important implications. First, it provides a solution to the variation in processing images. It may be difficult to universalize the processing of a microscopy scan, MRI scan, or any other type of medical image, but it is a lot easier to export a software that many institutions and labs can utilize. Additionally, different scans may introduce different biases. Neural style transfer may help eliminate certain biases. And most importantly, neural style transfer can lead to higher scoring of medical images. This means better diagnoses and less error when it comes to using ML for medical images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
